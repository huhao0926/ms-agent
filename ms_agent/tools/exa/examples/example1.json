{
  "data": {
    "requestId": "651609d63a5b23beaab2f1eb47399daf",
    "autopromptString": " latest methods about Agent RL in 3 months",
    "resolvedSearchType": "neural",
    "results": [
      {
        "id": "https://arxiv.org/abs/2505.20023",
        "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking",
        "url": "https://arxiv.org/abs/2505.20023",
        "publishedDate": "2025-05-26T00:00:00.000Z",
        "author": "[Submitted on 26 May 2025]",
        "score": 0.3673744797706604,
        "text": "\n View PDF \n HTML (experimental) \n Abstract: Autonomous agents, which perceive environments and take actions to achieve goals, have become increasingly feasible with the advancements in large language models (LLMs). However, current powerful agents often depend on sophisticated prompt engineering combined with closed-source LLMs like GPT-4. Although training open-source LLMs using expert trajectories from teacher models has yielded some improvements in agent capabilities, this approach still faces limitations such as performance plateauing and error propagation. To mitigate these challenges, we propose STeP, a novel method for improving LLM-based agent training. We synthesize self-reflected trajectories that include reflections and corrections of error steps, which enhance the effectiveness of LLM agents in learning from teacher models, enabling them to become agents capable of self-reflecting and correcting. We also introduce partial masking strategy that prevents the LLM from internalizing incorrect or suboptimal steps. Experiments demonstrate that our method improves agent performance across three representative tasks: ALFWorld, WebShop, and SciWorld. For the open-source model LLaMA2-7B-Chat, when trained using self-reflected trajectories constructed with Qwen1.5-110B-Chat as the teacher model, it achieves comprehensive improvements with less training data compared to agents trained exclusively on expert trajectories.\n \n \n Submission history From: Yihan Chen [ view email] [v1] \nMon, 26 May 2025 14:11:12 UTC (662 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      },
      {
        "id": "https://arxiv.org/abs/2505.10978",
        "title": "Group-in-Group Policy Optimization for LLM Agent Training",
        "url": "https://arxiv.org/abs/2505.10978",
        "publishedDate": "2025-05-16T00:00:00.000Z",
        "author": "[Submitted on 16 May 2025]",
        "score": 0.36698776483535767,
        "text": "\n View PDF \n HTML (experimental) \n Abstract: Recent advances in group-based reinforcement learning (RL) have driven frontier large language models (LLMs) in single-turn tasks like mathematical reasoning. However, their scalability to long-horizon LLM agent training remains limited. Unlike static tasks, agent-environment interactions unfold over many steps and often yield sparse or delayed rewards, making credit assignment across individual steps significantly more challenging. In this work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. GiGPO introduces a two-level structure for estimating relative advantage: (i) At the episode-level, GiGPO computes macro relative advantages based on groups of complete trajectories; (ii) At the step-level, GiGPO introduces an anchor state grouping mechanism that retroactively constructs step-level groups by identifying repeated environment states across trajectories. Actions stemming from the same state are grouped together, enabling micro relative advantage estimation. This hierarchical structure effectively captures both global trajectory quality and local step effectiveness without relying on auxiliary models or additional rollouts. We evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers fine-grained per-step credit signals and achieves performance gains of &gt; 12\\% on ALFWorld and &gt; 9\\% on WebShop over the GRPO baseline: all while maintaining the same GPU memory overhead, identical LLM rollout, and incurring little to no additional time cost.\n \n \n Submission history From: Lang Feng [ view email] [v1] \nFri, 16 May 2025 08:26:59 UTC (1,479 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      },
      {
        "id": "https://arxiv.org/abs/2505.20732",
        "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution",
        "url": "https://arxiv.org/abs/2505.20732",
        "publishedDate": "2025-05-27T00:00:00.000Z",
        "author": "[Submitted on 27 May 2025]",
        "score": 0.3639698326587677,
        "text": "\n View PDF \n HTML (experimental) \n Abstract: Reinforcement learning (RL) holds significant promise for training LLM agents to handle complex, goal-oriented tasks that require multi-step interactions with external environments. However, a critical challenge when applying RL to these agentic tasks arises from delayed rewards: feedback signals are typically available only after the entire task is completed. This makes it non-trivial to assign delayed rewards to earlier actions, providing insufficient guidance regarding environmental constraints and hindering agent training. In this work, we draw on the insight that the ultimate completion of a task emerges from the cumulative progress an agent makes across individual steps. We propose Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion. To achieve this, we train a progress estimator that accumulates stepwise contributions over a trajectory to match the task completion. During policy optimization, we combine the estimated per-step contribution with a grounding signal for actions executed in the environment as the fine-grained, intermediate reward for effective agent training. Extensive experiments on common agent benchmarks (including Webshop, ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the state-of-the-art method in both success rate (+2.5\\% on average) and grounding accuracy (+1.9\\% on average). Further analyses demonstrate that our method remarkably provides more effective intermediate rewards for RL training. Our code is available at this https URL.\n \n \n Submission history From: Hanlin Wang [ view email] [v1] \nTue, 27 May 2025 05:21:04 UTC (217 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      },
      {
        "id": "https://arxiv.org/abs/2505.07773",
        "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving",
        "url": "https://arxiv.org/abs/2505.07773",
        "publishedDate": "2025-05-12T00:00:00.000Z",
        "author": "[Submitted on 12 May 2025]",
        "score": 0.3575108051300049,
        "text": "\n View PDF \n HTML (experimental) \n Abstract: Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{ this https URL}{ this https URL}.\n \n \n Submission history From: Xinji Mai [ view email] [v1] \nMon, 12 May 2025 17:23:34 UTC (425 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      },
      {
        "id": "https://arxiv.org/abs/2505.17673",
        "title": "Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution",
        "url": "https://arxiv.org/abs/2505.17673",
        "publishedDate": "2025-05-23T00:00:00.000Z",
        "author": "[Submitted on 23 May 2025]",
        "score": 0.3561485707759857,
        "text": "\n View PDF \n HTML (experimental) \n Abstract: Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose tasks, define workflows, and assign agents to execute each step. While effective on benchmark-style tasks, such systems rely on designer updates and overlook agents' potential to learn from experience. Recently, Silver and Sutton(2025) envision a shift into a new era, where agents could progress from a stream of experiences. In this paper, we instantiate this vision of experience-driven learning by introducing a bottom-up agent paradigm that mirrors the human learning process. Agents acquire competence through a trial-and-reasoning mechanism-exploring, reflecting on outcomes, and abstracting skills over time. Once acquired, skills can be rapidly shared and extended, enabling continual evolution rather than static replication. As more agents are deployed, their diverse experiences accelerate this collective process, making bottom-up design especially suited for open-ended environments. We evaluate this paradigm in Slay the Spire and Civilization V, where agents perceive through raw visual inputs and act via mouse outputs, the same as human players. Using a unified, game-agnostic codebase without any game-specific prompts or privileged APIs, our bottom-up agents acquire skills entirely through autonomous interaction, demonstrating the potential of the bottom-up paradigm in complex, real-world environments. Our code is available at this https URL.\n \n \n Submission history From: Jiawei Du [ view email] [v1] \nFri, 23 May 2025 09:38:55 UTC (9,627 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      },
      {
        "id": "https://arxiv.org/abs/2505.13909",
        "title": "Efficient Agent Training for Computer Use",
        "url": "https://arxiv.org/abs/2505.13909",
        "publishedDate": "2025-05-20T00:00:00.000Z",
        "author": "[Submitted on 20 May 2025]",
        "score": 0.3544098436832428,
        "text": "\n View PDF \n HTML (experimental) \n Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.\n \n \n Submission history From: Yanheng He [ view email] [v1] \nTue, 20 May 2025 04:20:18 UTC (3,191 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      },
      {
        "id": "https://arxiv.org/abs/2505.20686",
        "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression",
        "url": "https://arxiv.org/abs/2505.20686",
        "publishedDate": "2025-05-27T00:00:00.000Z",
        "author": "[Submitted on 27 May 2025]",
        "score": 0.35272499918937683,
        "text": "\n View PDF \n HTML (experimental) \n Abstract: Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A$*-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V$*, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A$*-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\\times$ and peak memory usage by over 30% compared to PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at this https URL.\n \n \n Submission history From: Zhaolin Gao [ view email] [v1] \nTue, 27 May 2025 03:58:50 UTC (2,724 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      },
      {
        "id": "https://arxiv.org/abs/2505.19630",
        "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue",
        "url": "https://arxiv.org/abs/2505.19630",
        "publishedDate": "2025-05-26T00:00:00.000Z",
        "author": "[Submitted on 26 May 2025]",
        "score": 0.3492835462093353,
        "text": "\n View PDF \n HTML (experimental) \n Abstract: Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. this https URL \n \n \n Submission history From: Jiawei Wang [ view email] [v1] \nMon, 26 May 2025 07:48:14 UTC (11,905 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      },
      {
        "id": "https://arxiv.org/abs/2505.16421",
        "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning",
        "url": "https://arxiv.org/abs/2505.16421",
        "publishedDate": "2025-05-22T00:00:00.000Z",
        "author": "[Submitted on 22 May 2025]",
        "score": 0.7941938042640686,
        "text": "\n Authors: Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, Lihong Li \n View PDF \n HTML (experimental) \n Abstract: While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.\n \n \n Submission history From: Zhepei Wei [ view email] [v1] \nThu, 22 May 2025 09:07:43 UTC (8,459 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      },
      {
        "id": "https://arxiv.org/abs/2505.20285",
        "title": "MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability",
        "url": "https://arxiv.org/abs/2505.20285",
        "publishedDate": "2025-05-26T00:00:00.000Z",
        "author": "[Submitted on 26 May 2025]",
        "score": 0.3491935133934021,
        "text": "\n View PDF \n HTML (experimental) \n Abstract: Retrieval-Augmented Language Models (RALMs) represent a classic paradigm where models enhance generative capabilities using external knowledge retrieved via a specialized module. Recent advancements in Agent techniques enable Large Language Models (LLMs) to autonomously utilize tools for retrieval, planning, and reasoning. While existing training-based methods show promise, their agentic abilities are limited by inherent characteristics of the task-specific data used during training. To further enhance the universal search capability of agents, we propose a novel pre-training framework, MASKSEARCH. In the pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to leverage search tools to fill masked spans on a large number of pre-training data, thus acquiring universal retrieval and reasoning capabilities for LLMs. After that, the model is trained on downstream tasks to achieve further improvement. We apply both Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) for training. For SFT, we combine agent-based and distillation-based methods to generate training data, starting with a multi-agent system consisting of a planner, rewriter, observer, and followed by a self-evolving teacher model. While for RL, we employ DAPO as the training framework and adopt a hybrid reward system consisting of answer rewards and format rewards. Additionally, we introduce a curriculum learning approach that allows the model to learn progressively from easier to more challenging instances based on the number of masked spans. We evaluate the effectiveness of our framework in the scenario of open-domain multi-hop question answering. Through extensive experiments, we demonstrate that MASKSEARCH significantly enhances the performance of LLM-based search agents on both in-domain and out-of-domain downstream tasks.\n \n \n Submission history From: Xin Guan [ view email] [v1] \nMon, 26 May 2025 17:58:50 UTC (1,077 KB) \n",
        "image": "/static/browse/0.3.4/images/arxiv-logo-fb.png",
        "favicon": "https://arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png"
      }
    ],
    "costDollars": {
      "total": 0.015,
      "search": {
        "neural": 0.005
      },
      "contents": {
        "text": 0.01
      }
    }
  }
}
